{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNl8G3vHkPSX"
      },
      "source": [
        "# **Maestr√≠a en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## Curso: **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "### Tecnol√≥gico de Monterrey\n",
        "\n",
        "### Prof Luis Eduardo Falc√≥n Morales\n",
        "\n",
        "## Actividad Semana 7\n",
        "\n",
        "### Modelado de t√≥picos - LSI/LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69mHA6i201G"
      },
      "source": [
        "#### **Nombres y matr√≠culas de los integrantes del equipo:**\n",
        "\n",
        "\n",
        "\n",
        "*   A01795061 - Ricardo Burboa Astorga\n",
        "*   A01795353 - Alejandro Calder√≥n Aguilar\n",
        "*   A00278401 - Luis Antonio Calder√≥n Mata\n",
        "*   A01794243 - Erick Alexei Cambray Serv√≠n\n",
        "*   A01795086 - Juan Carlos Campero Villa\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wCL2p6MA8NuT"
      },
      "outputs": [],
      "source": [
        "# Aqu√≠ deber√°s incluir todas las librer√≠as que requieras durante esta actividad:\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SpanishStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import multiprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN24HsuGlfaF"
      },
      "source": [
        "Using pip install fasttext-wheel instead solved the problem for me."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c34ZOnna3Gu"
      },
      "source": [
        "##**Pregunta - 1:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeNllxRdmeWg"
      },
      "source": [
        "1. Descarga el archivo noticiasTopicModeling.txt que se encuentra en Canvas. Este archivo consiste en\n",
        "5658 noticias de varios peri√≥dicos de Espa√±a. El archivo de texto es una lista en el siguiente\n",
        "formato:\n",
        " [{‚Äútitular‚Äù:‚ÄùEncabezado‚Äú, ‚Äútexto‚Äù:‚ÄùCuerpo‚Äù}, ‚Ä¶ , {‚Äútitular‚Äù:‚ÄùEncabezado‚Äù,‚Äùtexto‚Äù:‚ÄùCuerpo‚Äù}]\n",
        "Donde ‚Äútitular‚Äù es el encabezado de la noticia y ‚Äútexto‚Äù es el cuerpo del texto de dicha noticia. En\n",
        "particular en esta actividad trabajar√°s solamente con los cuerpos de las noticias, sin incluir los\n",
        "encabezados. Carga dicho archivo y genera un DataFrame de Pandas llamado ‚Äúdf‚Äù y que contiene\n",
        "una √∫nica columna llamada ‚Äúnoticia‚Äù con 5658 renglones formados por los cuerpos de las noticias. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = r'Topic Modeling Activity.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T_lyEFRkxzC6",
        "outputId": "f02dbd6c-be35-463f-f27f-fb812fef0ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             noticia\n",
            "0  \"Espa√±a ha dejado de ser cat√≥lica\", dec√≠a Manu...\n",
            "1  El clima de crispaci√≥n social en Ceuta ha lleg...\n",
            "2  El Gobierno ha alegado la suspensi√≥n de plazos...\n",
            "3  Puedes mandar tu pregunta, sugerencia o queja¬†...\n",
            "4  Panam√° debe entregar esta semana a la Corte In...\n"
          ]
        }
      ],
      "source": [
        "# Leer el archivo\n",
        "with open(DATA_PATH, 'r', encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Convertir el contenido del archivo a una lista de diccionarios\n",
        "noticias = json.loads(data)\n",
        "\n",
        "# Extraer los cuerpos de las noticias\n",
        "cuerpos = [noticia['texto'] for noticia in noticias]\n",
        "\n",
        "# Crear el DataFrame\n",
        "df = pd.DataFrame(cuerpos, columns=['noticia'])\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame para verificar\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 2:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realiza un proceso de limpieza. Aplica el preprocesamiento que consideres adecuado para texto en\n",
        "espa√±ol. Recuerda que el objetivo es identificar los tokens (palabras) que describan mejor la\n",
        "distribuci√≥n de cada tema.\n",
        "NOTA: Recuerda que esta es una t√©cnica no supervisada, por lo que no requerimos hacer una\n",
        "partici√≥n de los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/alexeieacs/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Cargar modelo de spaCy para el espa√±ol\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Descargar y configurar stopwords adicionales de NLTK en espa√±ol\n",
        "nltk.download('stopwords')\n",
        "stop_words_spacy = nlp.Defaults.stop_words\n",
        "stop_words_nltk = set(stopwords.words('spanish'))\n",
        "combined_stopwords = stop_words_spacy.union(stop_words_nltk)\n",
        "\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    \"\"\"\n",
        "    Funci√≥n para limpiar y preprocesar texto en espa√±ol:\n",
        "    - Convierte el texto a min√∫sculas para homogeneizar la capitalizaci√≥n.\n",
        "    - Elimina caracteres no alfab√©ticos y n√∫meros para obtener solo palabras.\n",
        "    - Utiliza spaCy para analizar el texto y extraer tokens v√°lidos.\n",
        "    - Filtra los tokens eliminando stopwords y conservando solo palabras alfab√©ticas.\n",
        "    - Devuelve el texto limpio como una cadena de texto con palabras separadas por espacio.\n",
        "    \"\"\"\n",
        "    # Convertir a min√∫sculas\n",
        "    texto = texto.lower()\n",
        "    # Eliminar caracteres especiales y n√∫meros\n",
        "    texto = re.sub(r'[\\W_]+', ' ', texto)\n",
        "    texto = re.sub(r'\\d+', '', texto)\n",
        "    # Procesar texto con spaCy\n",
        "    doc = nlp(texto)\n",
        "    # Filtrar tokens\n",
        "    tokens = [\n",
        "        token.text for token in doc\n",
        "        if token.text not in combined_stopwords and token.is_alpha\n",
        "    ]\n",
        "    # Unir tokens en un string limpio\n",
        "    texto_limpio = ' '.join(tokens)\n",
        "    return texto_limpio\n",
        "\n",
        "\n",
        "# Aplicar la funci√≥n de limpieza a la columna 'noticia' de un DataFrame\n",
        "# Asumiendo que 'df' es un DataFrame y 'noticia' es la columna a procesar\n",
        "df['noticia'] = df['noticia'].apply(limpiar_texto)\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame para verificar la limpieza\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 3:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Encontrar la matriz Tf-idf de la columna de noticias. Despliega los primeros 5 renglones con\n",
        "algunas de sus columnas con sus nombres, donde las columnas son los tokens. ¬øCu√°l es el\n",
        "significado de cada rengl√≥n? ¬øY el significado de cada columna?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paso 1: Inicializar el vectorizador TF-IDF\n",
        "# El vectorizador TF-IDF permite convertir una colecci√≥n de documentos de texto brutos en una matriz de caracter√≠sticas TF-IDF.\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Paso 2: Ajustar el vectorizador y transformar los datos\n",
        "# 'fit_transform' aprende el vocabulario del corpus y transforma los datos en una matriz de caracter√≠sticas documentales.\n",
        "tfidf_matrix = vectorizer.fit_transform(df['noticia'])\n",
        "\n",
        "# Paso 3: Convertir la matriz TF-IDF en un DataFrame para una mejor manipulaci√≥n y visualizaci√≥n\n",
        "# Utilizamos 'toarray()' para convertir los resultados de la matriz esparsa a una matriz densa.\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),\n",
        "                        columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Paso 4: Mostrar las primeras filas del DataFrame resultante para verificar y visualizar los datos\n",
        "# Seleccionamos las primeras 5 filas y las primeras 10 columnas como muestra para entender c√≥mo se representan los datos.\n",
        "print(tfidf_df.iloc[:5, :10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explicaci√≥n de la Matriz TF-IDF\n",
        "\n",
        "### Rengl√≥n (Fila)\n",
        "Cada **rengl√≥n** en la matriz TF-IDF representa una **noticia** del conjunto de datos. En el DataFrame resultante, cada rengl√≥n contiene los valores TF-IDF de los tokens (palabras) presentes en esa noticia. Estos valores miden tanto la frecuencia de una palabra en una noticia espec√≠fica, como la importancia de la palabra a lo largo de todo el corpus (conjunto de todas las noticias), ayudando a identificar t√©rminos que son distintivamente relevantes en cada noticia.\n",
        "\n",
        "### Columna\n",
        "Cada **columna** en la matriz representa un **token** (palabra) del vocabulario generado a partir de todo el corpus de noticias. El valor en una celda espec√≠fica de la matriz TF-IDF indica la importancia de ese token en la noticia correspondiente. Este valor se calcula utilizando la frecuencia del t√©rmino (cu√°ntas veces aparece el t√©rmino en la noticia) ponderada por la frecuencia inversa del documento (cu√°n raro es el t√©rmino en todo el corpus de noticias). De esta manera, t√©rminos que aparecen frecuentemente en pocas noticias pero son raros en el corpus general reciben una mayor puntuaci√≥n, destacando su importancia en las noticias donde aparecen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 4:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aplica el m√©todo de descomposici√≥n de valores singulares truncado a la matriz Tf-idf anterior con\n",
        "10 componentes y obtener el gr√°fico de la importancia relativa de estas. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplicar Truncated SVD\n",
        "n_components = 10\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "svd_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Obtener la importancia relativa de cada componente\n",
        "explained_variance = svd.explained_variance_ratio_\n",
        "\n",
        "# Graficar la importancia relativa de los componentes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, n_components + 1),\n",
        "        explained_variance,\n",
        "        alpha=0.7,\n",
        "        align='center')\n",
        "plt.title('Importancia Relativa de los Componentes SVD')\n",
        "plt.xlabel('Componentes')\n",
        "plt.ylabel('Importancia Relativa')\n",
        "plt.xticks(range(1, n_components + 1))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 5:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtener la matriz tokens-temas (term-topic) a partir de la matriz ùëâ\n",
        "‡Øç\n",
        " de la descomposici√≥n SVD.\n",
        "Despliega sus primeros 5 renglones donde se incluya el nombre de las columnas. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener la matriz V^T\n",
        "Vt_matrix = svd.components_\n",
        "\n",
        "# Convertir la matriz V^T a un DataFrame\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "term_topic_df = pd.DataFrame(\n",
        "    Vt_matrix.T,\n",
        "    index=terms,\n",
        "    columns=[f'Tema {i+1}' for i in range(n_components)])\n",
        "\n",
        "# Mostrar los primeros 5 renglones del DataFrame\n",
        "print(term_topic_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 6:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con base a la cantidad de conceptos latentes que determinaste en el ejercicio anterior, obtener cada\n",
        "uno de sus gr√°ficos con sus 10 t√©rminos/tokens m√°s importantes. ¬øC√≥mo describir√≠as cada uno de\n",
        "dichos conceptos latentes? ¬øSe identifican claramente las tem√°ticas de cada uno de ellos? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para graficar los 10 t√©rminos m√°s importantes para cada tema\n",
        "def plot_top_terms_per_topic(term_topic_df, num_terms=10):\n",
        "    for topic in term_topic_df.columns:\n",
        "        top_terms = term_topic_df[topic].nlargest(num_terms)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.barh(top_terms.index, top_terms.values, color='skyblue')\n",
        "        plt.xlabel('Importancia')\n",
        "        plt.ylabel('T√©rminos')\n",
        "        plt.title(f'T√©rminos m√°s importantes en {topic}')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Graficar los 10 t√©rminos m√°s importantes para cada uno de los 10 temas\n",
        "plot_top_terms_per_topic(term_topic_df)\n",
        "\n",
        "# Describir cada uno de los temas\n",
        "for i, topic in enumerate(term_topic_df.columns, 1):\n",
        "    top_terms = term_topic_df[topic].nlargest(10).index\n",
        "    print(f\"Tema {i}: {', '.join(top_terms)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Descripci√≥n de los Temas del Corpus\n",
        "\n",
        "### Tema 1: Gesti√≥n del Coronavirus\n",
        "- **Palabras Claves**: coronavirus, gobierno, madrid, casos, pp, personas, espa√±a, datos, covid, pandemia\n",
        "- **Descripci√≥n**: Este tema probablemente aborda la gesti√≥n y las respuestas del gobierno y partidos pol√≠ticos como el PP frente a la pandemia de coronavirus en Espa√±a, incluyendo estad√≠sticas y casos espec√≠ficos en Madrid.\n",
        "\n",
        "### Tema 2: Seguimiento Epidemiol√≥gico\n",
        "- **Palabras Claves**: casos, coronavirus, covid, datos, municipio, evoluci√≥n, pandemia, gr√°ficos, rebrotes, provincias\n",
        "- **Descripci√≥n**: Este tema se centra en el an√°lisis epidemiol√≥gico del COVID-19, presentando datos, gr√°ficos y la evoluci√≥n de la pandemia a nivel municipal y provincial, incluyendo rebrotes.\n",
        "\n",
        "### Tema 3: Pol√≠tica en el Pa√≠s Vasco\n",
        "- **Palabras Claves**: euskadi, pnv, puedes, vasco, urkullu, osakidetza, contenidos, √°lava, bildu, eh\n",
        "- **Descripci√≥n**: Este tema trata sobre la pol√≠tica en el Pa√≠s Vasco, destacando la actividad de partidos como el PNV y Bildu, y figuras pol√≠ticas como Urkullu, adem√°s de mencionar la sanidad regional (Osakidetza).\n",
        "\n",
        "### Tema 4: Pol√≠tica Municipal y COVID-19\n",
        "- **Palabras Claves**: pp, municipio, casado, datos, covid, rebrotes, provincias, mapa, partido, municipios\n",
        "- **Descripci√≥n**: Explora la interacci√≥n entre la pol√≠tica municipal, especialmente del partido PP liderado por Casado, con la gesti√≥n de la pandemia, incluyendo mapas y datos de rebrotes en varias provincias.\n",
        "\n",
        "### Tema 5: Caso Villarejo en Municipios\n",
        "- **Palabras Claves**: villarejo, municipio, polic√≠a, comisario, datos, rebrotes, provincias, covid, mapa, juez\n",
        "- **Descripci√≥n**: Discute el impacto y las implicaciones del caso del comisario Villarejo en diversos municipios, incluyendo aspectos judiciales y conexiones con la pandemia.\n",
        "\n",
        "### Tema 6: Villarejo y la Pol√≠tica Madrile√±a\n",
        "- **Palabras Claves**: villarejo, madrid, polic√≠a, comisario, ayuso, sanidad, casos, comunidad, d√≠az, b√°rcenas\n",
        "- **Descripci√≥n**: Centrado en Madrid, este tema aborda las relaciones entre el comisario Villarejo, pol√≠ticos locales como Ayuso, y otros esc√°ndalos pol√≠ticos como el caso B√°rcenas.\n",
        "\n",
        "### Tema 7: Aspectos Econ√≥micos de la Pandemia\n",
        "- **Palabras Claves**: euros, madrid, pnv, erte, montai, ayuso, coronavirus, empresas, gobierno, millones\n",
        "- **Descripci√≥n**: Trata sobre las consecuencias econ√≥micas de la pandemia en Madrid y otras regiones, incluyendo ERTEs, ayudas a empresas y la gesti√≥n financiera del gobierno y partidos como el PNV.\n",
        "\n",
        "### Tema 8: Educaci√≥n y Pol√≠tica\n",
        "- **Palabras Claves**: m√°ster, cifuentes, alumnos, educaci√≥n, casado, curso, municipio, universidad, centros, profesores\n",
        "- **Descripci√≥n**: Enfocado en la educaci√≥n, este tema discute controversias como el caso del m√°ster de Cifuentes, implicaciones pol√≠ticas relacionadas con Casado, y la gesti√≥n educativa en universidades y municipios.\n",
        "\n",
        "### Tema 9: Informaci√≥n y Medios\n",
        "- **Palabras Claves**: coronavirus, casado, m√°ster, bolet√≠n, informaci√≥n, cifuentes, eldiario, hazte, √°lvarez, rey\n",
        "- **Descripci√≥n**: Analiza c√≥mo los medios y publicaciones como eldiario.es tratan temas de actualidad como el coronavirus, la pol√≠tica, y esc√°ndalos educativos, influenciando la opini√≥n p√∫blica.\n",
        "\n",
        "### Tema 10: Realeza y Finanzas\n",
        "- **Palabras Claves**: casos, rey, millones, carlos, juan, euros, em√©rito, m√°ster, pa√≠ses, muertes\n",
        "- **Descripci√≥n**: Este tema podr√≠a explorar las implic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Pregunta - 7:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utiliza la librer√≠a Gensim para implementar ahora la t√©cnica de LDA. Revisa la documentaci√≥n\n",
        "correspondiente y aplica de preferencia el modelo paralelizable:\n",
        "https://radimrehurek.com/gensim/models/ldamodel.html\n",
        "https://radimrehurek.com/gensim/models/ldamulticore.html \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "\n",
        "# Aseg√∫rate de descargar los recursos de NLTK si a√∫n no lo has hecho\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# Configurar el lematizador y stopwords\n",
        "stop_words = set(\n",
        "    stopwords.words('spanish'))  # Cambiar el idioma seg√∫n sea necesario\n",
        "stemmer = SpanishStemmer()\n",
        "\n",
        "\n",
        "# Funci√≥n para limpiar y preprocesar texto, devolviendo tokens\n",
        "def limpiar_texto_and_return_tokens(texto):\n",
        "    texto = texto.lower()  # Convertir a min√∫sculas\n",
        "    texto = re.sub(\n",
        "        r'[\\W_]+', ' ',\n",
        "        texto)  # Eliminar caracteres especiales y signos de puntuaci√≥n\n",
        "    texto = re.sub(r'\\d+', '', texto)  # Eliminar n√∫meros\n",
        "    tokens = word_tokenize(texto)  # Tokenizaci√≥n\n",
        "    tokens = [\n",
        "        stemmer.stem(token) for token in tokens if token not in stop_words\n",
        "    ]  # Lematizaci√≥n y eliminaci√≥n de stopwords\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Aplicar la funci√≥n de limpieza y tokenizaci√≥n a la columna 'noticia'\n",
        "df['tokens'] = df['noticia'].apply(limpiar_texto_and_return_tokens)\n",
        "\n",
        "# Crear el diccionario y el corpus de bolsa de palabras\n",
        "dictionary = corpora.Dictionary(df['tokens'])\n",
        "corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]\n",
        "\n",
        "# Configurar y aplicar el modelo LDA Multicore\n",
        "num_topics = 10\n",
        "lda_model = LdaMulticore(corpus,\n",
        "                         num_topics=num_topics,\n",
        "                         id2word=dictionary,\n",
        "                         passes=10,\n",
        "                         workers=multiprocessing.cpu_count() - 1)\n",
        "\n",
        "# Imprimir los temas identificados por LDA\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f'Tema {idx + 1}: {topic}')\n",
        "\n",
        "\n",
        "# Funci√≥n para visualizar los t√©rminos m√°s importantes por tema\n",
        "def plot_top_terms_per_topic_lda(lda_model, num_terms=10):\n",
        "    for i in range(num_topics):\n",
        "        terms = lda_model.show_topic(i, num_terms)\n",
        "        terms_df = pd.DataFrame(terms, columns=['Term', 'Weight'])\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.barh(terms_df['Term'], terms_df['Weight'], color='skyblue')\n",
        "        plt.xlabel('Importancia')\n",
        "        plt.ylabel('T√©rminos')\n",
        "        plt.title(f'T√©rminos m√°s importantes en Tema {i + 1}')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Llamar a la funci√≥n de visualizaci√≥n\n",
        "plot_top_terms_per_topic_lda(lda_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Pregunta - 8:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con base a esta t√©cnica ¬øqu√© cantidad de t√≥picos consideras que es la m√°s adecuada? Compara tus\n",
        "resultados con el m√©todo LSI. ¬øQu√© encuentras de coincidencias y diferencias? ¬øCu√°l consideras\n",
        "puede ser el mejor resultado, es decir, cu√°l consideras puede ser la mejor cantidad de t√≥picos a\n",
        "considerar?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funci√≥n para calcular la coherencia de t√≥picos para diferentes n√∫meros de temas\n",
        "def compute_coherence_values(dictionary,\n",
        "                             corpus,\n",
        "                             texts,\n",
        "                             limit,\n",
        "                             start=2,\n",
        "                             step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaMulticore(corpus,\n",
        "                             num_topics=num_topics,\n",
        "                             id2word=dictionary,\n",
        "                             passes=10,\n",
        "                             workers=multiprocessing.cpu_count() - 1)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model,\n",
        "                                        texts=texts,\n",
        "                                        dictionary=dictionary,\n",
        "                                        coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "\n",
        "# Calcular la coherencia para diferentes n√∫meros de temas\n",
        "limit = 30\n",
        "start = 2\n",
        "step = 2\n",
        "model_list, coherence_values = compute_coherence_values(\n",
        "    dictionary, corpus, df['tokens'], limit, start, step)\n",
        "\n",
        "# Graficar los resultados\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"N√∫mero de T√≥picos\")\n",
        "plt.ylabel(\"Coherencia de T√≥picos\")\n",
        "plt.title(\"Coherencia de T√≥picos para LDA\")\n",
        "plt.show()\n",
        "\n",
        "# Mostrar el n√∫mero √≥ptimo de t√≥picos\n",
        "optimal_num_topics = x[coherence_values.index(max(coherence_values))]\n",
        "print(f\"N√∫mero √≥ptimo de t√≥picos: {optimal_num_topics}\")\n",
        "\n",
        "# Comparar con LSI\n",
        "lsi_model = LsiModel(corpus, id2word=dictionary, num_topics=optimal_num_topics)\n",
        "lsi_coherence_model = CoherenceModel(model=lsi_model,\n",
        "                                     texts=df['tokens'],\n",
        "                                     dictionary=dictionary,\n",
        "                                     coherence='c_v')\n",
        "lsi_coherence = lsi_coherence_model.get_coherence()\n",
        "\n",
        "print(f\"Coherencia de T√≥picos para LSI: {lsi_coherence}\")\n",
        "print(f\"Coherencia de T√≥picos para LDA: {max(coherence_values)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### N√∫mero √ìptimo de T√≥picos:\n",
        "\n",
        "El n√∫mero √≥ptimo de t√≥picos se determina basado en la coherencia de los temas. Si la coherencia de LDA es significativamente mayor que la de LSI, LDA puede ser preferible.\n",
        "##### Coincidencias y Diferencias:\n",
        "\n",
        "LDA: Tiende a generar temas m√°s coherentes y especializados.\n",
        "LSI: Puede generar temas m√°s difusos y generales.\n",
        "##### Mejor Cantidad de T√≥picos:\n",
        "\n",
        "Basado en la coherencia, la cantidad de t√≥picos √≥ptima ser√≠a aquella que maximiza la coherencia en la gr√°fica de LDA.\n",
        "Si la coherencia de LDA es mayor que la de LSI, entonces LDA con el n√∫mero √≥ptimo de t√≥picos es preferible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 9:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Incluye tus conclusiones finales de la actividad. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
