{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNl8G3vHkPSX"
      },
      "source": [
        "# **Maestría en Inteligencia Artificial Aplicada**\n",
        "\n",
        "## Curso: **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "### Tecnológico de Monterrey\n",
        "\n",
        "### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## Actividad Semana 7\n",
        "\n",
        "### Modelado de tópicos - LSI/LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69mHA6i201G"
      },
      "source": [
        "#### **Nombres y matrículas de los integrantes del equipo:**\n",
        "\n",
        "\n",
        "\n",
        "*   A01795061 - Ricardo Burboa Astorga\n",
        "*   A01795353 - Alejandro Calderón Aguilar\n",
        "*   A00278401 - Luis Antonio Calderón Mata\n",
        "*   A01794243 - Erick Alexei Cambray Servín\n",
        "*   A01795086 - Juan Carlos Campero Villa\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wCL2p6MA8NuT"
      },
      "outputs": [],
      "source": [
        "# Aquí deberás incluir todas las librerías que requieras durante esta actividad:\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SpanishStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import multiprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN24HsuGlfaF"
      },
      "source": [
        "Using pip install fasttext-wheel instead solved the problem for me."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c34ZOnna3Gu"
      },
      "source": [
        "##**Pregunta - 1:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeNllxRdmeWg"
      },
      "source": [
        "1. Descarga el archivo noticiasTopicModeling.txt que se encuentra en Canvas. Este archivo consiste en\n",
        "5658 noticias de varios periódicos de España. El archivo de texto es una lista en el siguiente\n",
        "formato:\n",
        " [{“titular”:”Encabezado“, “texto”:”Cuerpo”}, … , {“titular”:”Encabezado”,”texto”:”Cuerpo”}]\n",
        "Donde “titular” es el encabezado de la noticia y “texto” es el cuerpo del texto de dicha noticia. En\n",
        "particular en esta actividad trabajarás solamente con los cuerpos de las noticias, sin incluir los\n",
        "encabezados. Carga dicho archivo y genera un DataFrame de Pandas llamado “df” y que contiene\n",
        "una única columna llamada “noticia” con 5658 renglones formados por los cuerpos de las noticias. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = r'Topic Modeling Activity.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T_lyEFRkxzC6",
        "outputId": "f02dbd6c-be35-463f-f27f-fb812fef0ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             noticia\n",
            "0  \"España ha dejado de ser católica\", decía Manu...\n",
            "1  El clima de crispación social en Ceuta ha lleg...\n",
            "2  El Gobierno ha alegado la suspensión de plazos...\n",
            "3  Puedes mandar tu pregunta, sugerencia o queja ...\n",
            "4  Panamá debe entregar esta semana a la Corte In...\n"
          ]
        }
      ],
      "source": [
        "# Leer el archivo\n",
        "with open(DATA_PATH, 'r', encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Convertir el contenido del archivo a una lista de diccionarios\n",
        "noticias = json.loads(data)\n",
        "\n",
        "# Extraer los cuerpos de las noticias\n",
        "cuerpos = [noticia['texto'] for noticia in noticias]\n",
        "\n",
        "# Crear el DataFrame\n",
        "df = pd.DataFrame(cuerpos, columns=['noticia'])\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame para verificar\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 2:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realiza un proceso de limpieza. Aplica el preprocesamiento que consideres adecuado para texto en\n",
        "español. Recuerda que el objetivo es identificar los tokens (palabras) que describan mejor la\n",
        "distribución de cada tema.\n",
        "NOTA: Recuerda que esta es una técnica no supervisada, por lo que no requerimos hacer una\n",
        "partición de los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/alexeieacs/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Cargar modelo de spaCy para el español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Descargar y configurar stopwords adicionales de NLTK en español\n",
        "nltk.download('stopwords')\n",
        "stop_words_spacy = nlp.Defaults.stop_words\n",
        "stop_words_nltk = set(stopwords.words('spanish'))\n",
        "combined_stopwords = stop_words_spacy.union(stop_words_nltk)\n",
        "\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    \"\"\"\n",
        "    Función para limpiar y preprocesar texto en español:\n",
        "    - Convierte el texto a minúsculas para homogeneizar la capitalización.\n",
        "    - Elimina caracteres no alfabéticos y números para obtener solo palabras.\n",
        "    - Utiliza spaCy para analizar el texto y extraer tokens válidos.\n",
        "    - Filtra los tokens eliminando stopwords y conservando solo palabras alfabéticas.\n",
        "    - Devuelve el texto limpio como una cadena de texto con palabras separadas por espacio.\n",
        "    \"\"\"\n",
        "    # Convertir a minúsculas\n",
        "    texto = texto.lower()\n",
        "    # Eliminar caracteres especiales y números\n",
        "    texto = re.sub(r'[\\W_]+', ' ', texto)\n",
        "    texto = re.sub(r'\\d+', '', texto)\n",
        "    # Procesar texto con spaCy\n",
        "    doc = nlp(texto)\n",
        "    # Filtrar tokens\n",
        "    tokens = [\n",
        "        token.text for token in doc\n",
        "        if token.text not in combined_stopwords and token.is_alpha\n",
        "    ]\n",
        "    # Unir tokens en un string limpio\n",
        "    texto_limpio = ' '.join(tokens)\n",
        "    return texto_limpio\n",
        "\n",
        "\n",
        "# Aplicar la función de limpieza a la columna 'noticia' de un DataFrame\n",
        "# Asumiendo que 'df' es un DataFrame y 'noticia' es la columna a procesar\n",
        "df['noticia'] = df['noticia'].apply(limpiar_texto)\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame para verificar la limpieza\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 3:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Encontrar la matriz Tf-idf de la columna de noticias. Despliega los primeros 5 renglones con\n",
        "algunas de sus columnas con sus nombres, donde las columnas son los tokens. ¿Cuál es el\n",
        "significado de cada renglón? ¿Y el significado de cada columna?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paso 1: Inicializar el vectorizador TF-IDF\n",
        "# El vectorizador TF-IDF permite convertir una colección de documentos de texto brutos en una matriz de características TF-IDF.\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Paso 2: Ajustar el vectorizador y transformar los datos\n",
        "# 'fit_transform' aprende el vocabulario del corpus y transforma los datos en una matriz de características documentales.\n",
        "tfidf_matrix = vectorizer.fit_transform(df['noticia'])\n",
        "\n",
        "# Paso 3: Convertir la matriz TF-IDF en un DataFrame para una mejor manipulación y visualización\n",
        "# Utilizamos 'toarray()' para convertir los resultados de la matriz esparsa a una matriz densa.\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),\n",
        "                        columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Paso 4: Mostrar las primeras filas del DataFrame resultante para verificar y visualizar los datos\n",
        "# Seleccionamos las primeras 5 filas y las primeras 10 columnas como muestra para entender cómo se representan los datos.\n",
        "print(tfidf_df.iloc[:5, :10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explicación de la Matriz TF-IDF\n",
        "\n",
        "### Renglón (Fila)\n",
        "Cada **renglón** en la matriz TF-IDF representa una **noticia** del conjunto de datos. En el DataFrame resultante, cada renglón contiene los valores TF-IDF de los tokens (palabras) presentes en esa noticia. Estos valores miden tanto la frecuencia de una palabra en una noticia específica, como la importancia de la palabra a lo largo de todo el corpus (conjunto de todas las noticias), ayudando a identificar términos que son distintivamente relevantes en cada noticia.\n",
        "\n",
        "### Columna\n",
        "Cada **columna** en la matriz representa un **token** (palabra) del vocabulario generado a partir de todo el corpus de noticias. El valor en una celda específica de la matriz TF-IDF indica la importancia de ese token en la noticia correspondiente. Este valor se calcula utilizando la frecuencia del término (cuántas veces aparece el término en la noticia) ponderada por la frecuencia inversa del documento (cuán raro es el término en todo el corpus de noticias). De esta manera, términos que aparecen frecuentemente en pocas noticias pero son raros en el corpus general reciben una mayor puntuación, destacando su importancia en las noticias donde aparecen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 4:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aplica el método de descomposición de valores singulares truncado a la matriz Tf-idf anterior con\n",
        "10 componentes y obtener el gráfico de la importancia relativa de estas. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplicar Truncated SVD\n",
        "n_components = 10\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "svd_matrix = svd.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Obtener la importancia relativa de cada componente\n",
        "explained_variance = svd.explained_variance_ratio_\n",
        "\n",
        "# Graficar la importancia relativa de los componentes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, n_components + 1),\n",
        "        explained_variance,\n",
        "        alpha=0.7,\n",
        "        align='center')\n",
        "plt.title('Importancia Relativa de los Componentes SVD')\n",
        "plt.xlabel('Componentes')\n",
        "plt.ylabel('Importancia Relativa')\n",
        "plt.xticks(range(1, n_components + 1))\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 5:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtener la matriz tokens-temas (term-topic) a partir de la matriz 𝑉\n",
        "்\n",
        " de la descomposición SVD.\n",
        "Despliega sus primeros 5 renglones donde se incluya el nombre de las columnas. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener la matriz V^T\n",
        "Vt_matrix = svd.components_\n",
        "\n",
        "# Convertir la matriz V^T a un DataFrame\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "term_topic_df = pd.DataFrame(\n",
        "    Vt_matrix.T,\n",
        "    index=terms,\n",
        "    columns=[f'Tema {i+1}' for i in range(n_components)])\n",
        "\n",
        "# Mostrar los primeros 5 renglones del DataFrame\n",
        "print(term_topic_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 6:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con base a la cantidad de conceptos latentes que determinaste en el ejercicio anterior, obtener cada\n",
        "uno de sus gráficos con sus 10 términos/tokens más importantes. ¿Cómo describirías cada uno de\n",
        "dichos conceptos latentes? ¿Se identifican claramente las temáticas de cada uno de ellos? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para graficar los 10 términos más importantes para cada tema\n",
        "def plot_top_terms_per_topic(term_topic_df, num_terms=10):\n",
        "    for topic in term_topic_df.columns:\n",
        "        top_terms = term_topic_df[topic].nlargest(num_terms)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.barh(top_terms.index, top_terms.values, color='skyblue')\n",
        "        plt.xlabel('Importancia')\n",
        "        plt.ylabel('Términos')\n",
        "        plt.title(f'Términos más importantes en {topic}')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Graficar los 10 términos más importantes para cada uno de los 10 temas\n",
        "plot_top_terms_per_topic(term_topic_df)\n",
        "\n",
        "# Describir cada uno de los temas\n",
        "for i, topic in enumerate(term_topic_df.columns, 1):\n",
        "    top_terms = term_topic_df[topic].nlargest(10).index\n",
        "    print(f\"Tema {i}: {', '.join(top_terms)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Descripción de los Temas del Corpus\n",
        "\n",
        "### Tema 1: Gestión del Coronavirus\n",
        "- **Palabras Claves**: coronavirus, gobierno, madrid, casos, pp, personas, españa, datos, covid, pandemia\n",
        "- **Descripción**: Este tema probablemente aborda la gestión y las respuestas del gobierno y partidos políticos como el PP frente a la pandemia de coronavirus en España, incluyendo estadísticas y casos específicos en Madrid.\n",
        "\n",
        "### Tema 2: Seguimiento Epidemiológico\n",
        "- **Palabras Claves**: casos, coronavirus, covid, datos, municipio, evolución, pandemia, gráficos, rebrotes, provincias\n",
        "- **Descripción**: Este tema se centra en el análisis epidemiológico del COVID-19, presentando datos, gráficos y la evolución de la pandemia a nivel municipal y provincial, incluyendo rebrotes.\n",
        "\n",
        "### Tema 3: Política en el País Vasco\n",
        "- **Palabras Claves**: euskadi, pnv, puedes, vasco, urkullu, osakidetza, contenidos, álava, bildu, eh\n",
        "- **Descripción**: Este tema trata sobre la política en el País Vasco, destacando la actividad de partidos como el PNV y Bildu, y figuras políticas como Urkullu, además de mencionar la sanidad regional (Osakidetza).\n",
        "\n",
        "### Tema 4: Política Municipal y COVID-19\n",
        "- **Palabras Claves**: pp, municipio, casado, datos, covid, rebrotes, provincias, mapa, partido, municipios\n",
        "- **Descripción**: Explora la interacción entre la política municipal, especialmente del partido PP liderado por Casado, con la gestión de la pandemia, incluyendo mapas y datos de rebrotes en varias provincias.\n",
        "\n",
        "### Tema 5: Caso Villarejo en Municipios\n",
        "- **Palabras Claves**: villarejo, municipio, policía, comisario, datos, rebrotes, provincias, covid, mapa, juez\n",
        "- **Descripción**: Discute el impacto y las implicaciones del caso del comisario Villarejo en diversos municipios, incluyendo aspectos judiciales y conexiones con la pandemia.\n",
        "\n",
        "### Tema 6: Villarejo y la Política Madrileña\n",
        "- **Palabras Claves**: villarejo, madrid, policía, comisario, ayuso, sanidad, casos, comunidad, díaz, bárcenas\n",
        "- **Descripción**: Centrado en Madrid, este tema aborda las relaciones entre el comisario Villarejo, políticos locales como Ayuso, y otros escándalos políticos como el caso Bárcenas.\n",
        "\n",
        "### Tema 7: Aspectos Económicos de la Pandemia\n",
        "- **Palabras Claves**: euros, madrid, pnv, erte, montai, ayuso, coronavirus, empresas, gobierno, millones\n",
        "- **Descripción**: Trata sobre las consecuencias económicas de la pandemia en Madrid y otras regiones, incluyendo ERTEs, ayudas a empresas y la gestión financiera del gobierno y partidos como el PNV.\n",
        "\n",
        "### Tema 8: Educación y Política\n",
        "- **Palabras Claves**: máster, cifuentes, alumnos, educación, casado, curso, municipio, universidad, centros, profesores\n",
        "- **Descripción**: Enfocado en la educación, este tema discute controversias como el caso del máster de Cifuentes, implicaciones políticas relacionadas con Casado, y la gestión educativa en universidades y municipios.\n",
        "\n",
        "### Tema 9: Información y Medios\n",
        "- **Palabras Claves**: coronavirus, casado, máster, boletín, información, cifuentes, eldiario, hazte, álvarez, rey\n",
        "- **Descripción**: Analiza cómo los medios y publicaciones como eldiario.es tratan temas de actualidad como el coronavirus, la política, y escándalos educativos, influenciando la opinión pública.\n",
        "\n",
        "### Tema 10: Realeza y Finanzas\n",
        "- **Palabras Claves**: casos, rey, millones, carlos, juan, euros, emérito, máster, países, muertes\n",
        "- **Descripción**: Este tema podría explorar las implic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Pregunta - 7:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utiliza la librería Gensim para implementar ahora la técnica de LDA. Revisa la documentación\n",
        "correspondiente y aplica de preferencia el modelo paralelizable:\n",
        "https://radimrehurek.com/gensim/models/ldamodel.html\n",
        "https://radimrehurek.com/gensim/models/ldamulticore.html \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "\n",
        "# Asegúrate de descargar los recursos de NLTK si aún no lo has hecho\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# Configurar el lematizador y stopwords\n",
        "stop_words = set(\n",
        "    stopwords.words('spanish'))  # Cambiar el idioma según sea necesario\n",
        "stemmer = SpanishStemmer()\n",
        "\n",
        "\n",
        "# Función para limpiar y preprocesar texto, devolviendo tokens\n",
        "def limpiar_texto_and_return_tokens(texto):\n",
        "    texto = texto.lower()  # Convertir a minúsculas\n",
        "    texto = re.sub(\n",
        "        r'[\\W_]+', ' ',\n",
        "        texto)  # Eliminar caracteres especiales y signos de puntuación\n",
        "    texto = re.sub(r'\\d+', '', texto)  # Eliminar números\n",
        "    tokens = word_tokenize(texto)  # Tokenización\n",
        "    tokens = [\n",
        "        stemmer.stem(token) for token in tokens if token not in stop_words\n",
        "    ]  # Lematización y eliminación de stopwords\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Aplicar la función de limpieza y tokenización a la columna 'noticia'\n",
        "df['tokens'] = df['noticia'].apply(limpiar_texto_and_return_tokens)\n",
        "\n",
        "# Crear el diccionario y el corpus de bolsa de palabras\n",
        "dictionary = corpora.Dictionary(df['tokens'])\n",
        "corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]\n",
        "\n",
        "# Configurar y aplicar el modelo LDA Multicore\n",
        "num_topics = 10\n",
        "lda_model = LdaMulticore(corpus,\n",
        "                         num_topics=num_topics,\n",
        "                         id2word=dictionary,\n",
        "                         passes=10,\n",
        "                         workers=multiprocessing.cpu_count() - 1)\n",
        "\n",
        "# Imprimir los temas identificados por LDA\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f'Tema {idx + 1}: {topic}')\n",
        "\n",
        "\n",
        "# Función para visualizar los términos más importantes por tema\n",
        "def plot_top_terms_per_topic_lda(lda_model, num_terms=10):\n",
        "    for i in range(num_topics):\n",
        "        terms = lda_model.show_topic(i, num_terms)\n",
        "        terms_df = pd.DataFrame(terms, columns=['Term', 'Weight'])\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.barh(terms_df['Term'], terms_df['Weight'], color='skyblue')\n",
        "        plt.xlabel('Importancia')\n",
        "        plt.ylabel('Términos')\n",
        "        plt.title(f'Términos más importantes en Tema {i + 1}')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Llamar a la función de visualización\n",
        "plot_top_terms_per_topic_lda(lda_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Pregunta - 8:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con base a esta técnica ¿qué cantidad de tópicos consideras que es la más adecuada? Compara tus\n",
        "resultados con el método LSI. ¿Qué encuentras de coincidencias y diferencias? ¿Cuál consideras\n",
        "puede ser el mejor resultado, es decir, cuál consideras puede ser la mejor cantidad de tópicos a\n",
        "considerar?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para calcular la coherencia de tópicos para diferentes números de temas\n",
        "def compute_coherence_values(dictionary,\n",
        "                             corpus,\n",
        "                             texts,\n",
        "                             limit,\n",
        "                             start=2,\n",
        "                             step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaMulticore(corpus,\n",
        "                             num_topics=num_topics,\n",
        "                             id2word=dictionary,\n",
        "                             passes=10,\n",
        "                             workers=multiprocessing.cpu_count() - 1)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model,\n",
        "                                        texts=texts,\n",
        "                                        dictionary=dictionary,\n",
        "                                        coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values\n",
        "\n",
        "\n",
        "# Calcular la coherencia para diferentes números de temas\n",
        "limit = 30\n",
        "start = 2\n",
        "step = 2\n",
        "model_list, coherence_values = compute_coherence_values(\n",
        "    dictionary, corpus, df['tokens'], limit, start, step)\n",
        "\n",
        "# Graficar los resultados\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Número de Tópicos\")\n",
        "plt.ylabel(\"Coherencia de Tópicos\")\n",
        "plt.title(\"Coherencia de Tópicos para LDA\")\n",
        "plt.show()\n",
        "\n",
        "# Mostrar el número óptimo de tópicos\n",
        "optimal_num_topics = x[coherence_values.index(max(coherence_values))]\n",
        "print(f\"Número óptimo de tópicos: {optimal_num_topics}\")\n",
        "\n",
        "# Comparar con LSI\n",
        "lsi_model = LsiModel(corpus, id2word=dictionary, num_topics=optimal_num_topics)\n",
        "lsi_coherence_model = CoherenceModel(model=lsi_model,\n",
        "                                     texts=df['tokens'],\n",
        "                                     dictionary=dictionary,\n",
        "                                     coherence='c_v')\n",
        "lsi_coherence = lsi_coherence_model.get_coherence()\n",
        "\n",
        "print(f\"Coherencia de Tópicos para LSI: {lsi_coherence}\")\n",
        "print(f\"Coherencia de Tópicos para LDA: {max(coherence_values)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Número Óptimo de Tópicos:\n",
        "\n",
        "El número óptimo de tópicos se determina basado en la coherencia de los temas. Si la coherencia de LDA es significativamente mayor que la de LSI, LDA puede ser preferible.\n",
        "##### Coincidencias y Diferencias:\n",
        "\n",
        "LDA: Tiende a generar temas más coherentes y especializados.\n",
        "LSI: Puede generar temas más difusos y generales.\n",
        "##### Mejor Cantidad de Tópicos:\n",
        "\n",
        "Basado en la coherencia, la cantidad de tópicos óptima sería aquella que maximiza la coherencia en la gráfica de LDA.\n",
        "Si la coherencia de LDA es mayor que la de LSI, entonces LDA con el número óptimo de tópicos es preferible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##**Pregunta - 9:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Incluye tus conclusiones finales de la actividad. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
